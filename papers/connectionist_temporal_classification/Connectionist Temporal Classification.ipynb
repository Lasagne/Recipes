{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation the Connectionist Temporal Classification loss function:\n",
    "\n",
    "> Graves, A., Fern√°ndez, S., Gomez, F., & Schmidhuber, J. (2006, June). Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning (pp. 369-376). ACM. ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf\n",
    "\n",
    "This notebook only show the learning procedure, no thorough testing is performed and the prefix search decoding is not implemented (contributions are welcome!).\n",
    "\n",
    "The original paper seems to use size 1 minibatches instead of 16 here. There shouldn't be any significant variations otherwise.\n",
    "\n",
    "Please download the [TIMIT dataset](http://academictorrents.com/details/34e2b78745138186976cbc27939b1b34d18bd5b3) and place the `TIMIT.zip` file next to this one.\n",
    "\n",
    "The following python packages are required:\n",
    "- scipy\n",
    "- lasagne\n",
    "- matplotlib\n",
    "- [sphfile](https://pypi.python.org/pypi/sphfile) (to read the sound files)\n",
    "- [python_speech_features](https://github.com/jameslyons/python_speech_features) (to generate mfcc features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda\"\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from sphfile import SPHFile\n",
    "from python_speech_features import mfcc\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, LSTMLayer, DenseLayer, ConcatLayer, GaussianNoiseLayer\n",
    "from lasagne.init import Uniform\n",
    "from lasagne.nonlinearities import tanh, sigmoid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from ctc import ctc_loss, log_softmax, ctc_backward\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## small useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, w):\n",
    "    window = int(np.ceil(len(x) / 2 * (1000 ** w - 1) / 999))\n",
    "    window += 1 - window % 2\n",
    "    \n",
    "    if window < 3 or len(x) < window:\n",
    "        return x\n",
    "    \n",
    "    edge_weights = np.arange(1, window // 2 + 1)\n",
    "    return np.concatenate([\n",
    "        np.cumsum(x[:window // 2]) / edge_weights,\n",
    "        np.convolve(x, np.full([window], 1 / window), 'valid'),\n",
    "        np.cumsum(x[:-window // 2:-1])[::-1] / edge_weights[::-1]])\n",
    "\n",
    "def argmax_decode(preds, exclude=()):\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    decoded = [preds[0]]\n",
    "    for v in preds:\n",
    "        if v != decoded[-1]:\n",
    "            decoded.append(v)\n",
    "    \n",
    "    return np.array([v for v in decoded if v not in exclude])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"data/lisa/data/timit/raw/TIMIT\"):\n",
    "    assert os.path.exists(\"TIMIT.zip\"), \"Missing data archive\"\n",
    "    with ZipFile(\"TIMIT.zip\", 'r') as f:\n",
    "        f.extractall(path=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "train_subset = []\n",
    "\n",
    "for dirpath, _, filenames in os.walk(\"data/lisa/data/timit/raw/TIMIT\"):\n",
    "    for f in filenames:\n",
    "        if f.endswith(\"WAV\"):\n",
    "            recording = SPHFile(dirpath + \"/\" + f).content\n",
    "            files.append(dirpath + \"/\" + f[:-4])\n",
    "            train_subset.append(dirpath[31:36] == \"TRAIN\")\n",
    "\n",
    "files = np.array(files)\n",
    "train_subset = np.array(train_subset, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"preprocessed_dataset.pkl\"):\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for f in files:\n",
    "        recording = SPHFile(f + \".WAV\")\n",
    "        signal = recording.content\n",
    "        samplerate = recording.format['sample_rate']\n",
    "\n",
    "        mfccfeats = mfcc(signal, samplerate=samplerate, winlen=0.01, winstep=0.005, \n",
    "                         numcep=13, nfilt=26, appendEnergy=True)\n",
    "        derivatives = np.concatenate([\n",
    "            mfccfeats[1, None] - mfccfeats[0, None],\n",
    "            .5 * mfccfeats[2:] - .5 * mfccfeats[0:-2],\n",
    "            mfccfeats[-1, None] - mfccfeats[-2, None]], axis=0)\n",
    "\n",
    "        features.append(np.concatenate([mfccfeats, derivatives], axis=1).astype(np.float32))\n",
    "\n",
    "        with open(f + \".PHN\") as phonem_file:\n",
    "            labels.append([l.split()[2] for l in phonem_file.readlines()])\n",
    "\n",
    "    m = np.mean(np.concatenate(features, axis=0))\n",
    "    s = np.std(np.concatenate(features, axis=0))\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - m) / s\n",
    "\n",
    "    vocabulary = set()\n",
    "    for lseq in labels:\n",
    "        vocabulary |= set(lseq)\n",
    "\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary[-1], vocabulary[vocabulary.index('h#')] = \\\n",
    "        vocabulary[vocabulary.index('h#')], vocabulary[-1]\n",
    "    blank = len(vocabulary) - 1\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = np.array([vocabulary.index(l) for l in labels[i]], dtype=np.int32)\n",
    "    \n",
    "    with open(\"preprocessed_dataset.pkl\", 'wb') as f:\n",
    "        pkl.dump((features, labels, vocabulary, blank), f, -1)\n",
    "\n",
    "\n",
    "with open(\"preprocessed_dataset.pkl\", 'rb') as f:\n",
    "    features, labels, vocabulary, blank = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's go brutal and shove that in GPU memory\n",
    "\n",
    "n_sequences = len(features)\n",
    "feat_size = features[0].shape[1]\n",
    "max_duration = max(len(seq) for seq in features)\n",
    "max_labels = max(len(seq) - 2 for seq in labels)  # -2 for init and final blank\n",
    "\n",
    "durations = np.array([len(seq) for seq in features], dtype=np.int32)\n",
    "nlabels = np.array([len(seq) - 2 for seq in labels], dtype=np.int32)\n",
    "all_features = np.zeros((n_sequences, max_duration, feat_size), dtype=np.float32)\n",
    "for i in range(n_sequences):\n",
    "    all_features[i, :durations[i]] = features[i]\n",
    "all_labels = np.zeros((n_sequences, max_labels), dtype=np.int32)\n",
    "for i in range(n_sequences):\n",
    "    all_labels[i, :nlabels[i]] = labels[i][1:-1]\n",
    "\n",
    "durations_var = T.as_tensor_variable(durations, name=\"durations\")\n",
    "all_features_var = T.as_tensor_variable(all_features, name=\"all_features\")\n",
    "nlabels_var = T.as_tensor_variable(nlabels, name=\"nlabels\")\n",
    "all_labels_var = T.as_tensor_variable(all_labels, name=\"all_labels\")\n",
    "\n",
    "minibatch_indexes = T.ivector()\n",
    "batch_features = all_features_var[minibatch_indexes]\n",
    "batch_durations = durations_var[minibatch_indexes]\n",
    "batch_nlabels = nlabels_var[minibatch_indexes]\n",
    "batch_labels = all_labels_var[minibatch_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "l_in = InputLayer(\n",
    "    input_var=batch_features,\n",
    "    shape=(batch_size, max_duration, feat_size))\n",
    "\n",
    "l_duration = InputLayer(input_var=batch_durations, shape=(1,))\n",
    "\n",
    "l_mask = lasagne.layers.ExpressionLayer(\n",
    "    l_duration, \n",
    "    lambda d: T.arange(max_duration)[None, :] < d[:, None])\n",
    "\n",
    "l_noise = GaussianNoiseLayer(l_in, sigma=0.6)\n",
    "# l_noise = l_in\n",
    "\n",
    "l_fwlstm = LSTMLayer(\n",
    "    l_noise, 100,\n",
    "    ingate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    forgetgate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    cell=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=tanh),\n",
    "    outgate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    nonlinearity=tanh,\n",
    "    mask_input=l_mask, peepholes=True)\n",
    "l_bwlstm = LSTMLayer(\n",
    "    l_noise, 100,\n",
    "    ingate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    forgetgate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    cell=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=tanh),\n",
    "    outgate=lasagne.layers.Gate(W_cell=Uniform(0.1), nonlinearity=sigmoid),\n",
    "    nonlinearity=tanh,\n",
    "    mask_input=l_mask, peepholes=True, backwards=True)\n",
    "\n",
    "l_cat = ConcatLayer([l_fwlstm, l_bwlstm], axis=2)\n",
    "\n",
    "l_linout = DenseLayer(\n",
    "    l_cat, len(vocabulary), \n",
    "    nonlinearity=None,\n",
    "    num_leading_axes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = lasagne.layers.get_output(\n",
    "    l_linout, deterministic=False).dimshuffle(1, 0, 2)\n",
    "\n",
    "loss = ctc_loss(\n",
    "    linout=train_output,\n",
    "    durations=batch_durations,\n",
    "    labels=batch_labels,\n",
    "    label_sizes=batch_nlabels,\n",
    "    blank=blank)\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_linout, trainable=True)\n",
    "grads = theano.grad(loss.sum(), params)\n",
    "updates = lasagne.updates.adam(\n",
    "    grads, params, \n",
    "    learning_rate=1e-4)\n",
    "update_fn = theano.function(\n",
    "    [minibatch_indexes], \n",
    "    loss,\n",
    "    updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "nsteps = int(100 * n_sequences / batch_size)\n",
    "params_history = []\n",
    "loss_history = np.zeros((nsteps,))\n",
    "\n",
    "def update_plot(fig, ax1, ax2, loss_history):\n",
    "    ax1.clear()\n",
    "    ax1.set_xlim(0, len(loss_history))\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_ylim(0.8 * np.percentile(loss_history, 1), \n",
    "                1.2 * np.percentile(loss_history, 99))\n",
    "    ax1.grid(color='gray', linestyle='-', linewidth=1)\n",
    "    ax1.grid(color='gray', linestyle=':', which='minor', linewidth=1)\n",
    "    ax1.set_axisbelow(True)\n",
    "    xticks = np.arange(len(loss_history))\n",
    "    ax1.scatter(xticks, loss_history, marker='.', \n",
    "               color='firebrick', edgecolor=\"none\", alpha=0.1)\n",
    "    smooth_history = smooth(loss_history, 0.6)\n",
    "    ax1.plot(xticks, smooth_history, linewidth=2, color='firebrick')\n",
    "\n",
    "    ax2.clear()\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_ylim(0.8 * np.percentile(loss_history, 1), \n",
    "                 1.2 * np.percentile(loss_history, 99))\n",
    "    ax2.grid(False)\n",
    "    ax2.yaxis.set_label_position(\"right\")\n",
    "    ax2.set_yticks([], minor=True)\n",
    "    ax2.set_yticks([smooth_history[-1]])\n",
    "    ax2.get_yaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "xticks = np.arange(i)\n",
    "ax1.set_xlim(0, i + 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Note: you can interrupt and resume the execution of this cell\n",
    "while i < nsteps:\n",
    "    t1 = time.time()\n",
    "    batch_loss = np.mean(update_fn(\n",
    "        np.random.choice(n_sequences, batch_size).astype(np.int32)))\n",
    "    t2 = time.time()\n",
    "    \n",
    "    print(\"\\r{:<6d} loss = {:>5.0f}, (d={:1.2f})\".format(i, batch_loss, t2 - t1), end='', flush=True)\n",
    "    loss_history[i] = batch_loss\n",
    "\n",
    "    if (i + 1) % 10 == 0:        \n",
    "        update_plot(fig, ax1, ax2, loss_history[:i])\n",
    "\n",
    "#     if (i + 1) % 1000 == 0:\n",
    "#         params_history.append(lasagne.layers.get_all_param_values(l_linout))\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = lasagne.layers.get_output(l_linout, deterministic=True)\n",
    "\n",
    "logits_fn = theano.function(\n",
    "    [minibatch_indexes],\n",
    "    [batch_features, batch_durations, \n",
    "     batch_labels, batch_nlabels, \n",
    "     test_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = 3\n",
    "\n",
    "f, d, l, n, p = logits_fn(np.array([sequence], dtype=np.int32))\n",
    "f = f[0, :d[0]]\n",
    "l = l[0, :n[0]]\n",
    "p = p[0, :d[0]]\n",
    "s = np.exp(p - np.max(p, axis=-1, keepdims=True)) \\\n",
    "    / np.sum(np.exp(p - np.max(p, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "lines = []\n",
    "\n",
    "for c in np.argsort(vocabulary[:-1]):\n",
    "    if c in l:\n",
    "        line, = ax.plot(np.arange(len(p)), s[:, c], label=vocabulary[c], picker=5)\n",
    "        lines.append(line)\n",
    "\n",
    "ax.plot(np.arange(len(p)), s[:, -1], linestyle=\":\")\n",
    "\n",
    "ax.set_ylim(0.0, 1.2)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_title('Select curve to see the label')\n",
    "\n",
    "ax.legend(\n",
    "    framealpha=1,\n",
    "    loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=8)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.5)\n",
    "fig.show()\n",
    "\n",
    "def onpick(event):\n",
    "    for line in lines:\n",
    "        line.set_alpha(0.3)\n",
    "        line.set_linewidth(2)\n",
    "    \n",
    "    event.artist.set_alpha(1)\n",
    "    event.artist.set_linewidth(2)\n",
    "    ax.set_title(event.artist.get_label())\n",
    "\n",
    "cid = fig.canvas.mpl_connect('pick_event', onpick)\n",
    "\n",
    "print(\"target    : {}\".format(\", \".join(vocabulary[l_] for l_ in l)))\n",
    "print(\"prediction: {}\".format(\", \".join(vocabulary[l_] for l_ in argmax_decode(s, [blank]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
