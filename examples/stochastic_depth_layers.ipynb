{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from theano import tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "from lasagne.nonlinearities import *\n",
    "from lasagne.objectives import *\n",
    "from lasagne.regularization import *\n",
    "from lasagne.random import get_rng\n",
    "from lasagne.updates import *\n",
    "from lasagne.init import *\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "from urllib import urlretrieve\n",
    "import cPickle as pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Binomial dropout layer\n",
    "\n",
    "   Samples a binomial mask on the first axis (i.e. batch size)\n",
    "   and multiplies it with the input. This has the effect of\n",
    "   zeroing the output for some examples in the batch (according\n",
    "   to the survival probability p)\n",
    "   \n",
    "   Parameters\n",
    "   ----------\n",
    "   \n",
    "   incoming : a :class:`Layer` instance\n",
    "   p : float\n",
    "       The survival probability for an example in the batch\n",
    "\n",
    "\"\"\"\n",
    "class BinomialDropLayer(Layer):\n",
    "    def __init__(self, incoming, p=0.5, **kwargs):\n",
    "        super(BinomialDropLayer, self).__init__(incoming, **kwargs)\n",
    "        self._srng = RandomStreams(get_rng().randint(1, 2147462579))\n",
    "        self.p = p\n",
    "\n",
    "    def get_output_for(self, input, deterministic=False, **kwargs):\n",
    "        if deterministic:\n",
    "            return self.p*input\n",
    "        else:\n",
    "            mask = self._srng.binomial(n=1, p=(self.p), size=(input.shape[0],),\n",
    "                dtype=input.dtype)\n",
    "            mask = mask.dimshuffle(0,'x','x','x')\n",
    "            return mask*input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   http://arxiv.org/abs/1603.09382\n",
    "   \n",
    "   \"...we replace the identity connections in these blocks\n",
    "   by an averaging pooling layer followed by zero paddings\n",
    "   to match the dimensions.\"\n",
    "   \n",
    "   To explain this method, let us consider two consecutive\n",
    "   convolution layers, `conv1` and `conv2`. Let us assume\n",
    "   they have different output shapes. To create the\n",
    "   identity connection between `conv1` and `conv2`, do 2x2\n",
    "   average pooling on `conv1`. Then, pad the result so that\n",
    "   it has the same dimensions as `conv2`. Afterwards, we have\n",
    "   to see if the final result has the same number of feature\n",
    "   maps as `conv2`; if not, we have to add all-zero feature\n",
    "   maps to either side of the result. Then, we construct a \n",
    "   binomial drop layer so that we can compute the final \n",
    "   equation:\n",
    "   \n",
    "   binomial_mask*conv2 + id(conv1)\n",
    "   \n",
    "   If we pass this through a nonlinearity layer, we can then\n",
    "   do: g( binomial_mask*conv2 + id(conv1) )\n",
    "   \n",
    "   Parameters\n",
    "   ----------\n",
    "   \n",
    "   incoming : a :class:`Layer` instance\n",
    "   p : float\n",
    "       The survival probability for the binomial mask\n",
    "\n",
    "\"\"\"\n",
    "def stochastic_depth_block(incoming, p, nonlinearity=linear):\n",
    "    layer_before_incoming = None\n",
    "    for prev_layer in get_all_layers(incoming)[::-1][1::]:\n",
    "        if \"ignore\" not in prev_layer.name and not isinstance(prev_layer, NonlinearityLayer):\n",
    "            layer_before_incoming = prev_layer\n",
    "            break\n",
    "    if layer_before_incoming == None:\n",
    "        raise Exception(\"Cannot find an appropriate layer before layer: %s\" % incoming.name)\n",
    "        \n",
    "    if layer_before_incoming.output_shape != incoming.output_shape:    \n",
    "        l_pool = Pool2DLayer(layer_before_incoming, pool_size=(2,2), mode=\"average_inc_pad\", name=\"ignore_pool\")\n",
    "        if (l_pool.output_shape[2] % 2 == 1 and incoming.output_shape[2] % 2 == 0) or \\\n",
    "            (l_pool.output_shape[2] % 2 == 0 and incoming.output_shape[2] % 2 == 1):\n",
    "            l_pad = pad( l_pool, width=((0,1),(0,1)), name=\"ignore_prelim_pad\" )\n",
    "        else:\n",
    "            l_pad = l_pool\n",
    "        nd1 = (incoming.output_shape[2]-l_pad.output_shape[2])/2\n",
    "        if nd1 > 0:\n",
    "            l_pad = pad(l_pad, width=(nd1,nd1), name=\"ignore_pad\")\n",
    "        # what if the layer_before_incoming num feature maps is\n",
    "        # less than the incoming_layer num feature maps?\n",
    "        if layer_before_incoming.output_shape[1] < incoming.output_shape[1]:\n",
    "            diff_in_fms = incoming.output_shape[1]-layer_before_incoming.output_shape[1]\n",
    "            if diff_in_fms % 2 == 0: \n",
    "                width_tp = ((diff_in_fms/2, diff_in_fms/2),)\n",
    "            else:\n",
    "                width_tp = (((diff_in_fms/2)+1, diff_in_fms/2),)\n",
    "            l_pad = pad(\n",
    "                l_pad, \n",
    "                batch_ndim=1, \n",
    "                width=width_tp,\n",
    "                name=\"ignore_fm_pad\"\n",
    "            )\n",
    "        l_binom_drop = BinomialDropLayer(incoming, p=p, name=\"ignore_binom\")\n",
    "        l_sum = ElemwiseSumLayer([l_binom_drop, l_pad], name=\"ignore_elemsum\")\n",
    "        l_nonlinearity = NonlinearityLayer(l_sum, nonlinearity=nonlinearity, name=\"ignore_nonlinearity\")\n",
    "        return l_nonlinearity\n",
    "    else:\n",
    "        l_binom_drop = BinomialDropLayer(incoming, p=p, name=\"ignore_binom\")\n",
    "        l_sum = ElemwiseSumLayer([l_binom_drop, layer_before_incoming], name=\"ignore_elemsum\")\n",
    "        l_nonlinearity = NonlinearityLayer(l_sum, nonlinearity=nonlinearity, name=\"ignore_nonlinearity\")\n",
    "        return l_nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a simple convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (None, 1, 28, 28)\n",
      "l_conv1 (None, 8, 26, 26)\n",
      "ignore_binom (None, 8, 26, 26)\n",
      "ignore_pool (None, 1, 14, 14)\n",
      "ignore_pad (None, 1, 26, 26)\n",
      "ignore_fm_pad (None, 8, 26, 26)\n",
      "ignore_elemsum (None, 8, 26, 26)\n",
      "ignore_nonlinearity (None, 8, 26, 26)\n",
      "l_mp1 (None, 8, 13, 13)\n",
      "ignore_binom (None, 8, 13, 13)\n",
      "ignore_pool (None, 8, 13, 13)\n",
      "ignore_elemsum (None, 8, 13, 13)\n",
      "ignore_nonlinearity (None, 8, 13, 13)\n",
      "l_conv2 (None, 8, 11, 11)\n",
      "ignore_binom (None, 8, 11, 11)\n",
      "ignore_pool (None, 8, 6, 6)\n",
      "ignore_prelim_pad (None, 8, 7, 7)\n",
      "ignore_pad (None, 8, 11, 11)\n",
      "ignore_elemsum (None, 8, 11, 11)\n",
      "ignore_nonlinearity (None, 8, 11, 11)\n",
      "l_mp2 (None, 8, 5, 5)\n",
      "ignore_binom (None, 8, 5, 5)\n",
      "ignore_pool (None, 8, 5, 5)\n",
      "ignore_elemsum (None, 8, 5, 5)\n",
      "ignore_nonlinearity (None, 8, 5, 5)\n",
      "l_conv3 (None, 16, 3, 3)\n",
      "ignore_binom (None, 16, 3, 3)\n",
      "ignore_pool (None, 8, 2, 2)\n",
      "ignore_prelim_pad (None, 8, 3, 3)\n",
      "ignore_fm_pad (None, 16, 3, 3)\n",
      "ignore_elemsum (None, 16, 3, 3)\n",
      "ignore_nonlinearity (None, 16, 3, 3)\n",
      "l_fc (None, 10)\n",
      "num of params: 3282\n"
     ]
    }
   ],
   "source": [
    "l_in = InputLayer( (None, 1, 28, 28), name=\"input\" )\n",
    "\n",
    "l_conv1 = Conv2DLayer(l_in, num_filters=8, filter_size=3, name=\"l_conv1\", nonlinearity=None)\n",
    "l_sd1 = stochastic_depth_block(l_conv1, p=0.5, nonlinearity=rectify)\n",
    "\n",
    "l_mp1 = MaxPool2DLayer(l_sd1, pool_size=(2,2), name=\"l_mp1\")\n",
    "l_sd2 = stochastic_depth_block(l_mp1, p=0.5)\n",
    "\n",
    "l_conv2 = Conv2DLayer(l_sd2, num_filters=8, filter_size=3, name=\"l_conv2\", nonlinearity=None)\n",
    "l_sd3 = stochastic_depth_block(l_conv2, p=0.5, nonlinearity=rectify)\n",
    "\n",
    "l_mp2 = MaxPool2DLayer(l_sd3, pool_size=(2,2), name=\"l_mp2\")\n",
    "l_sd4 = stochastic_depth_block(l_mp2, p=0.5)\n",
    "\n",
    "l_conv3 = Conv2DLayer(l_sd4, num_filters=16, filter_size=3, name=\"l_conv3\", nonlinearity=None)\n",
    "l_sd5 = stochastic_depth_block(l_conv3, p=0.5, nonlinearity=rectify)\n",
    "\n",
    "l_fc = DenseLayer(l_sd5, num_units=10, nonlinearity=softmax, name=\"l_fc\")\n",
    "\n",
    "l_out = l_fc\n",
    "\n",
    "for layer in get_all_layers(l_out):\n",
    "    print layer.name, layer.output_shape\n",
    "print \"num of params: %i\" % count_params(l_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's download the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_ret = urlretrieve(\"http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\", \"/tmp/mnist.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open(\"/tmp/mnist.pkl.gz\") as f:\n",
    "    dat = pickle.load(f)\n",
    "train_data, _, _ = dat\n",
    "X_train, y_train = train_data\n",
    "X_train = X_train.reshape( (X_train.shape[0], 1, 28, 28) ).astype( theano.config.floatX )\n",
    "y_train = y_train.astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 1, 28, 28), (50000,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Lasagne-related stuff we'll need for training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('X')\n",
    "y = T.ivector('y')\n",
    "\n",
    "net_out = get_output(l_out, X)\n",
    "net_out_det = get_output(l_out, X, deterministic=True)\n",
    "loss = categorical_crossentropy(net_out, y).mean()\n",
    "params = get_all_params(l_out, trainable=True)\n",
    "grads = T.grad(loss, params)\n",
    "updates = nesterov_momentum(grads, params, learning_rate=0.01, momentum=0.9)\n",
    "train_fn = theano.function(inputs=[X, y], outputs=loss, updates=updates)\n",
    "out_fn = theano.function(inputs=[X], outputs=net_out_det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.04170554822\n",
      "2 0.838029498493\n",
      "3 0.794951380409\n",
      "4 0.759657711097\n",
      "5 0.733128651309\n",
      "6 0.715255246341\n",
      "7 0.718663093263\n",
      "8 0.710821818392\n",
      "9 0.721151590125\n",
      "10 0.707448014221\n"
     ]
    }
   ],
   "source": [
    "bs = 32 \n",
    "n_batches = X_train.shape[0] // bs\n",
    "num_epochs = 10\n",
    "for epoch in range(0, num_epochs):\n",
    "    train_losses = []\n",
    "    for b in range(0, n_batches):\n",
    "        train_losses.append( train_fn(X_train[b*bs : (b+1)*bs], y_train[b*bs : (b+1)*bs]) )\n",
    "    print (epoch+1), np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What accuracy do we get on the training set? (Between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92969999999999997"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum( np.argmax( out_fn(X_train), axis=1 ) == y_train ) / float(X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
